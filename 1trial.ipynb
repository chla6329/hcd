{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras,cv2,os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "\n",
    "from tqdm import tqdm_notebook,trange\n",
    "from glob import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/kagglecomp/hcd\" \n",
    "traindata = path + 'train/'\n",
    "testdata = path + 'test/'\n",
    "\n",
    "submitname = \"Submission_009.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'path': glob(os.path.join(traindata,'*.tif'))})  \n",
    "labels = pd.read_csv(path+\"train_labels.csv\") \n",
    "df = df.merge(labels, on = \"id\")\n",
    "df.head(3) \n",
    "\n",
    "def load_data(N,df):  \n",
    "    X = np.zeros([N,96,96,3],dtype=np.uint8)   \n",
    "    for i, row in tqdm_notebook(df.iterrows(), total=N):\n",
    "        if i == N:\n",
    "            break\n",
    "        X[i] = cv2.imread(row['path'])\n",
    "          \n",
    "    return X,y\n",
    "\n",
    "N=20000\n",
    "X,y = load_data(N=N,df=df) \n",
    "\n",
    "fig = plt.figure(figsize=(10, 4), dpi=150)\n",
    "np.random.seed(100) \n",
    "for plotNr,idx in enumerate(np.random.randint(0,N,8)):\n",
    "    ax = fig.add_subplot(2, 8//2, plotNr+1, xticks=[], yticks=[])  \n",
    "    plt.imshow(X[idx])  \n",
    "    ax.set_title('Label: ' + str(y[idx]))  \n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(4, 2),dpi=150)\n",
    "plt.bar([1,0], [(y==0).sum(), (y==1).sum()]);  \n",
    "plt.xticks([1,0],[\"Negative (N={})\".format((y==0).sum()),\"Positive (N={})\".format((y==1).sum())]);\n",
    "plt.ylabel(\"# of smps\")\n",
    "\n",
    "positive_smps = X[y == 1]\n",
    "negative_smps = X[y == 0]\n",
    "\n",
    "nr_of_bins = 256 \n",
    "fig,axs = plt.subplots(4,2,sharey=True,figsize=(8,8),dpi=150)\n",
    " \n",
    " \n",
    "axs[0,0].set_title(\"Positive smps (N =\" + str(positive_smps.shape[0]) + \")\");\n",
    "axs[0,1].set_title(\"Negative smps (N =\" + str(negative_smps.shape[0]) + \")\");\n",
    "axs[0,1].set_ylabel(\"Red\",rotation='horizontal',labelpad=35,fontsize=12)\n",
    "axs[1,1].set_ylabel(\"Green\",rotation='horizontal',labelpad=35,fontsize=12)\n",
    "axs[2,1].set_ylabel(\"Blue\",rotation='horizontal',labelpad=35,fontsize=12)\n",
    "axs[3,1].set_ylabel(\"RGB\",rotation='horizontal',labelpad=35,fontsize=12)\n",
    "for i in range(4):\n",
    "    axs[i,0].set_ylabel(\"Relative frequency\")\n",
    "axs[3,0].set_xlabel(\"Pixel value\")\n",
    "axs[3,1].set_xlabel(\"Pixel value\")\n",
    "fig.tight_layout()\n",
    " \n",
    " \n",
    "N = df[\"path\"].size  \n",
    "X,y = load_data(N=N,df=df)\n",
    "\n",
    "training_portion = 0.8 \n",
    "split_idx = int(np.round(training_portion * y.shape[0]))  \n",
    "\n",
    "np.random.seed(42)  \n",
    " \n",
    "idx = np.arange(y.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "X = X[idx]\n",
    "y = y[idx] \n",
    "kernel_size = (3,3)\n",
    "pool_size= (2,2)\n",
    "first_filters = 32\n",
    "second_filters = 64\n",
    "third_filters = 128\n",
    "\n",
    " \n",
    "dropout_conv = 0.3\n",
    "dropout_dense = 0.5\n",
    " \n",
    "model = Sequential()\n",
    " \n",
    "model.add(Conv2D(first_filters, kernel_size, input_shape = (96, 96, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(first_filters, kernel_size, use_bias=False)) \n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size = pool_size)) \n",
    "model.add(Dropout(dropout_conv))\n",
    " \n",
    "model.add(Conv2D(second_filters, kernel_size, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(second_filters, kernel_size, use_bias=False)) \n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size = pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    " \n",
    "model.add(Conv2D(third_filters, kernel_size, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(third_filters, kernel_size, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size = pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(dropout_dense))\n",
    " \n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "batch_size = 50\n",
    " \n",
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(0.001), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "epochs = 3 \n",
    "for epoch in range(epochs): \n",
    "    iterations = np.floor(split_idx / batch_size).astype(int)  \n",
    "    with trange(iterations) as t:  \n",
    "        for i in t:\n",
    "            start_idx = i * batch_size  \n",
    "            x_batch = X[start_idx:start_idx+batch_size]  \n",
    "            y_batch = y[start_idx:start_idx+batch_size]  \n",
    "\n",
    "            metrics = model.train_on_batch(x_batch, y_batch)  \n",
    "\n",
    "            loss = loss + metrics[0]  \n",
    "            acc = acc + metrics[1]  \n",
    "            t.set_description('Running training epoch ' + str(epoch))  \n",
    "            t.set_postfix(loss=\"%.2f\" % round(loss / (i+1),2),acc=\"%.2f\" % round(acc / (i+1),2))  \n",
    "\n",
    " \n",
    "iterations = np.floor((y.shape[0]-split_idx) / batch_size).astype(int)  \n",
    "loss,acc = 0,0  \n",
    "with trange(iterations) as t:  \n",
    "    for i in t:\n",
    "        start_idx = i * batch_size  \n",
    "        x_batch = X[start_idx:start_idx+batch_size]  \n",
    "        y_batch = y[start_idx:start_idx+batch_size] \n",
    "        \n",
    "        metrics = model.test_on_batch(x_batch, y_batch)  \n",
    "        \n",
    "        loss = loss + metrics[0] \n",
    "        acc = acc + metrics[1]  \n",
    "        t.set_postfix(loss=\"%.2f\" % round(loss / (i+1),2),acc=\"%.2f\" % round(acc / (i+1),2))\n",
    "        \n",
    "print(\"Validation loss:\",loss / iterations) \n",
    "X = None\n",
    "y = None \n",
    "\n",
    "\n",
    "\n",
    "base_test_dir = path + 'test/' \n",
    "test_files = glob(os.path.join(base_test_dir,'*.tif')) \n",
    "submission = pd.DataFrame() \n",
    "file_batch = 5000  \n",
    "max_idx = len(test_files) \n",
    "for idx in range(0, max_idx, file_batch): \n",
    "    print(\"Indexes: %i - %i\"%(idx, idx+file_batch))\n",
    "    test_df = pd.DataFrame({'path': test_files[idx:idx+file_batch]}) \n",
    "    test_df['id'] = test_df.path.map(lambda x: x.split('/')[3].split(\".\")[0])  \n",
    "    test_df['image'] = test_df['path'].map(cv2.imread)  \n",
    "    K_test = np.stack(test_df[\"image\"].values)  \n",
    "    predictions = model.predict(K_test,verbose = 1)  \n",
    "    test_df['label'] = predictions  \n",
    "    submission = pd.concat([submission, test_df[[\"id\", \"label\"]]])\n",
    "submission.head()  \n",
    "\n",
    " \n",
    "submission.to_csv(submitname, index = False, header = True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
